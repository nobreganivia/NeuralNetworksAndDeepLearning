# Warm up: a fast matrix-based approach to computing the output from a neural network
Before discussing backpropagation, let's warm up with a fast matrix-based algorithm to compute the output from a neural network. We actually already briefly saw this algorithm near the end of the last chapter, but I described it quickly, so it's worth revisiting in detail. In particular, this is a good way of getting comfortable with the notation used in backpropagation, in a familiar context.

Let's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use <p align=""><img src="https://user-images.githubusercontent.com/78389645/140316513-8699054d-30a6-4562-9816-b9a8501d2c79.png" width="75"/><br></i></p> to denote the weight for the connection from the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140316984-9ce0f87e-85f1-4aa9-89d5-52a7a18eeba0.png" width="75"/><br></i></p> neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317217-a7dc468c-f924-4256-9156-e6c0fba4d3c5.png" width="100"/><br></i></p> layer to the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317507-5f86c0aa-cfef-4f36-bd85-8ae87b5d366d.png" width="75"/><br></i></p> neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer. So, for example, the diagram below shows the weight on a connection from the fourth neuron in the second layer to the second neuron in the third layer of a network:

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138961743-cb483343-9550-45ff-bfb8-8ccc258d4dfe.png" width="400"/><br></i>
</p>

This notation is cumbersome at first, and it does take some work to master. But with a little effort you'll find the notation becomes easy and natural. One quirk of the notation is the ordering of the j and k indices. You might think that it makes more sense to use j to refer to the input neuron, and k to the output neuron, not vice versa, as is actually done. I'll explain the reason for this quirk below.
We use a similar notation for the network's biases and activations. Explicitly, we use <p align=""><img src="https://user-images.githubusercontent.com/78389645/140318677-8e55e72f-a4df-475b-aa95-775d5d33ac95.png" width="55"/><br></i></p> for the bias of the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140318930-f4e925ba-ed93-4ddb-87c8-b6942cb1f789.png" width="70"/><br></i></p> neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer. And we use <p align=""><img src="https://user-images.githubusercontent.com/78389645/140319585-830ac198-8d55-4f43-ad50-157703b30ddb.png" width="75"/><br></i></p> for the activation of the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140318930-f4e925ba-ed93-4ddb-87c8-b6942cb1f789.png" width="70"/><br></i></p> neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer. The following diagram shows examples of these notations in use:

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138963039-353a04dc-f1cf-4117-b503-5f912096b111.png" width="350"/><br></i>
</p>

With these notations, the activation  <p align=""><img src="https://user-images.githubusercontent.com/78389645/140319585-830ac198-8d55-4f43-ad50-157703b30ddb.png" width="75"/><br></i></p> of the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140318930-f4e925ba-ed93-4ddb-87c8-b6942cb1f789.png" width="70"/><br></i></p> neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer is related to the activations in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317217-a7dc468c-f924-4256-9156-e6c0fba4d3c5.png" width="100"/><br></i></p> layer by the equation (compare Equation (4) and surrounding discussion in the last chapter)

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138963824-0f90070e-48ed-4f50-b88e-91aff8f06a95.png" width="350"/>
   <b><a name="23">(23)</a></b>
</p>

where the sum is over all neurons k in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317217-a7dc468c-f924-4256-9156-e6c0fba4d3c5.png" width="100"/><br></i></p> layer. To rewrite this expression in a matrix form we define a weight matrix <p align=""><img src="https://user-images.githubusercontent.com/78389645/140364320-af412b48-396c-413c-9710-32d767ab11e9.png" width="70"/><br></i></p> for each layer, l. The entries of the weight matrix <p align=""><img src="https://user-images.githubusercontent.com/78389645/140364320-af412b48-396c-413c-9710-32d767ab11e9.png" width="70"/><br></i></p> are just the weights connecting to the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer of neurons, that is, the entry in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317507-5f86c0aa-cfef-4f36-bd85-8ae87b5d366d.png" width="75"/><br></i></p> row and <p align=""><img src="https://user-images.githubusercontent.com/78389645/140373956-a4d719f1-b7b4-4269-952c-6394a5d75c6a.png" width="75"/><br></i></p> column is <p align=""><img src="https://user-images.githubusercontent.com/78389645/140316513-8699054d-30a6-4562-9816-b9a8501d2c79.png" width="75"/><br></i></p>. Similarly, for each layer l we define a bias vector, <p align=""><img src="https://user-images.githubusercontent.com/78389645/140375035-1a8e4969-ab01-4f6f-a74d-43b38c6ebdf7.png" width="75"/><br></i></p>. You can probably guess how this works - the components of the bias vector are just the values <p align=""><img src="https://user-images.githubusercontent.com/78389645/140375394-29b2630f-737b-4b65-a6da-5944f3b6763f.png" width="75"/><br></i></p>, one component for each neuron in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140317872-323c34a3-84f4-48ff-9875-5dd2866e2fb2.png" width="75"/><br></i></p> layer. And finally, we define an activation vector <p align=""><img src="https://user-images.githubusercontent.com/78389645/140378111-17b9b78a-9e15-4d2e-bf59-867854bb52d5.png" width="75"/><br></i></p> whose components are the activations <p align=""><img src="https://user-images.githubusercontent.com/78389645/140378353-3dd7ba6a-cfcd-431c-88b2-07afb2f29752.png" width="75"/><br></i></p>.

The last ingredient we need to rewrite (23) in a matrix form is the idea of vectorizing a function such as σ. We met vectorization briefly in the last chapter, but to recap, the idea is that we want to apply a function such as σ to every element in a vector v. We use the obvious notation σ(v) to denote this kind of elementwise application of a function. That is, the components of σ(v) are just <p align=""><img src="https://user-images.githubusercontent.com/78389645/140380727-eb95f8fc-c47f-49cb-a744-21810bd0abaa.png" width="100"/><br></i></p>. As an example, if we have the function f(x)=x^2 then the vectorized form of f has the effect

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138971713-99b1f814-fb9c-4bd6-a0ac-fbe2bdb29755.png" width="350"/>
   <b><a name="24">(24)</a></b>
</p>

that is, the vectorized f just squares every element of the vector.

With these notations in mind, Equation (23) can be rewritten in the beautiful and compact vectorized form

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138971911-3a218788-e8a1-4872-b4ac-18d9ea569327.png" width="350"/>
   <b><a name="25">(25)</a></b>
</p>

This expression gives us a much more global way of thinking about how the activations in one layer relate to activations in the previous layer: we just apply the weight matrix to the activations, then add the bias vector, and finally apply the σ function.[^1] That global view is often easier and more succinct (and involves fewer indices!) than the neuron-by-neuron view we've taken to now. Think of it as a way of escaping index hell, while remaining precise about what's going on. The expression is also useful in practice, because most matrix libraries provide fast ways of implementing matrix multiplication, vector addition, and vectorization. Indeed, the [code](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits) in the last chapter made implicit use of this expression to compute the behaviour of the network.

When using Equation (25) to compute <p align=""><img src="https://user-images.githubusercontent.com/78389645/140378111-17b9b78a-9e15-4d2e-bf59-867854bb52d5.png" width="75"/><br></i></p>, we compute the intermediate quantity <p align=""><img src="https://user-images.githubusercontent.com/78389645/140381326-f9173958-a28c-4d9f-8b08-4aca742fbaf1.png" width="150"/><br></i></p> along the way. This quantity turns out to be useful enough to be worth naming: we call <p align=""><img src="https://user-images.githubusercontent.com/78389645/140381679-044f866d-9a37-4aee-a134-007756bf8d9a.png" width="75"/><br></i></p> the weighted input to the neurons in layer l. We'll make considerable use of the weighted input <p align=""><img src="https://user-images.githubusercontent.com/78389645/140381679-044f866d-9a37-4aee-a134-007756bf8d9a.png" width="75"/><br></i></p> later in the chapter. Equation (25) is sometimes written in terms of the weighted input, as <p align=""><img src="https://user-images.githubusercontent.com/78389645/140382173-8cd823bf-baab-4667-86e2-5ffdd938e1b8.png" width="75"/><br></i></p>. It's also worth noting that <p align=""><img src="https://user-images.githubusercontent.com/78389645/140381679-044f866d-9a37-4aee-a134-007756bf8d9a.png" width="75"/><br></i></p> has components <p align=""><img src="https://user-images.githubusercontent.com/78389645/140382421-4173e914-3007-46d0-ba50-3b3e7160da58.png" width="150"/><br></i></p>, that is, <p align=""><img src="https://user-images.githubusercontent.com/78389645/140382881-722c32e5-9e68-4707-8298-4b6f84f8d82d.png" width="75"/><br></i></p> is just the weighted input to the activation function for neuron j in layer l.

[^1]: By the way, it's this expression that motivates the quirk in the <p align=""><img src="https://user-images.githubusercontent.com/78389645/140316513-8699054d-30a6-4562-9816-b9a8501d2c79.png" width="75"/><br></i></p> notation mentioned earlier. If we used j to index the input neuron, and k to index the output neuron, then we'd need to replace the weight matrix in Equation (25) by the transpose of the weight matrix. That's a small change, but annoying, and we'd lose the easy simplicity of saying (and thinking) "apply the weight matrix to the activations".
