# Warm up: a fast matrix-based approach to computing the output from a neural network
Before discussing backpropagation, let's warm up with a fast matrix-based algorithm to compute the output from a neural network. We actually already briefly saw this algorithm near the end of the last chapter, but I described it quickly, so it's worth revisiting in detail. In particular, this is a good way of getting comfortable with the notation used in backpropagation, in a familiar context.

Let's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use w^ljk to denote the weight for the connection from the k^th neuron in the (l−1)^th layer to the j^th neuron in the l^th layer. So, for example, the diagram below shows the weight on a connection from the fourth neuron in the second layer to the second neuron in the third layer of a network:

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138961743-cb483343-9550-45ff-bfb8-8ccc258d4dfe.png" width="400"/><br></i>
</p>

This notation is cumbersome at first, and it does take some work to master. But with a little effort you'll find the notation becomes easy and natural. One quirk of the notation is the ordering of the j and k indices. You might think that it makes more sense to use j to refer to the input neuron, and k to the output neuron, not vice versa, as is actually done. I'll explain the reason for this quirk below.
We use a similar notation for the network's biases and activations. Explicitly, we use b^lj for the bias of the j^th neuron in the l^th layer. And we use a^lj for the activation of the j^th neuron in the l^th layer. The following diagram shows examples of these notations in use:

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138963039-353a04dc-f1cf-4117-b503-5f912096b111.png" width="350"/><br></i>
</p>

With these notations, the activation a^lj of the j^th neuron in the l^th layer is related to the activations in the (l−1)^th layer by the equation (compare Equation (4) and surrounding discussion in the last chapter)

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138963824-0f90070e-48ed-4f50-b88e-91aff8f06a95.png" width="350"/>
   <b><a name="23">(23)</a></b>
</p>

where the sum is over all neurons k in the (l−1)^th layer. To rewrite this expression in a matrix form we define a weight matrix w^l for each layer, l. The entries of the weight matrix w^l are just the weights connecting to the l^th layer of neurons, that is, the entry in the j^th row and kth column is w^ljk. Similarly, for each layer l we define a bias vector, bl. You can probably guess how this works - the components of the bias vector are just the values b^lj, one component for each neuron in the l^th layer. And finally, we define an activation vector al whose components are the activations a^lj.
The last ingredient we need to rewrite (23) in a matrix form is the idea of vectorizing a function such as σ. We met vectorization briefly in the last chapter, but to recap, the idea is that we want to apply a function such as σ to every element in a vector v. We use the obvious notation σ(v) to denote this kind of elementwise application of a function. That is, the components of σ(v) are just σ(v)j=σ(vj). As an example, if we have the function f(x)=x^2 then the vectorized form of f has the effect

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138971713-99b1f814-fb9c-4bd6-a0ac-fbe2bdb29755.png" width="350"/>
   <b><a name="24">(24)</a></b>
</p>

that is, the vectorized f just squares every element of the vector.

With these notations in mind, Equation (23) can be rewritten in the beautiful and compact vectorized form

<p align="center">
  <img src="https://user-images.githubusercontent.com/78389645/138971911-3a218788-e8a1-4872-b4ac-18d9ea569327.png" width="350"/>
   <b><a name="25">(25)</a></b>
</p>

This expression gives us a much more global way of thinking about how the activations in one layer relate to activations in the previous layer: we just apply the weight matrix to the activations, then add the bias vector, and finally apply the σ function.[^1] That global view is often easier and more succinct (and involves fewer indices!) than the neuron-by-neuron view we've taken to now. Think of it as a way of escaping index hell, while remaining precise about what's going on. The expression is also useful in practice, because most matrix libraries provide fast ways of implementing matrix multiplication, vector addition, and vectorization. Indeed, the [code](http://neuralnetworksanddeeplearning.com/chap1.html#implementing_our_network_to_classify_digits) in the last chapter made implicit use of this expression to compute the behaviour of the network.

When using Equation (25) to compute a^l, we compute the intermediate quantity z^l≡[w^la^(l−1)]+b^l along the way. This quantity turns out to be useful enough to be worth naming: we call z^l the weighted input to the neurons in layer l. We'll make considerable use of the weighted input z^l later in the chapter. Equation (25) is sometimes written in terms of the weighted input, as a^l=σ(z^l). It's also worth noting that z^l has components z^lj=∑k wljk al−1k + blj, that is, z^lj is just the weighted input to the activation function for neuron j in layer l.

[^1]: By the way, it's this expression that motivates the quirk in the w^ljk notation mentioned earlier. If we used j to index the input neuron, and k to index the output neuron, then we'd need to replace the weight matrix in Equation (25) by the transpose of the weight matrix. That's a small change, but annoying, and we'd lose the easy simplicity of saying (and thinking) "apply the weight matrix to the activations".
